{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Week 5: Named Entity Extraction with BER"
      ],
      "metadata": {
        "id": "RKzCpMIE7k8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrn4AzDS4GOX",
        "outputId": "2d7f92b3-7f32-4bc1-f0be-b6f2c2320cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the necessary libraries\n"
      ],
      "metadata": {
        "id": "4GAsx88D8r-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zogPn-mo8oCV",
        "outputId": "5a19a561-46f1-42e2-b613-abbe9e32ac54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAV63Uly7Qdc",
        "outputId": "b24ccc99-f7d8-4b95-dcaa-e70037f3dfdf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kWmZK6J9QHp",
        "outputId": "d3ca0982-079e-4b71-b50f-1166dea6ffaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEFPajZW_UPU",
        "outputId": "d429a19c-e23c-4222-ab43-7f2436f57095"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Libraries"
      ],
      "metadata": {
        "id": "YpiG8Qbq86jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertForTokenClassification, get_linear_schedule_with_warmup\n",
        "from seqeval.metrics import classification_report"
      ],
      "metadata": {
        "id": "YIxcAVYM85x6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqACHvTY7p8W",
        "outputId": "f334d707-a221-44df-be4e-e9c324b59fa5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRL-h07I8OvE",
        "outputId": "b2a5d59c-795d-40c3-c14a-27614e8a619d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_0_str = \" \".join(dataset[\"train\"][0]['tokens'])\n",
        "sentence_0_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "moe0p55w8TKl",
        "outputId": "98499821-a45f-43d2-9bee-13cd93ba20a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EU rejects German call to boycott British lamb .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map pos tag numbers to their labels\n",
        "pos_tags = dataset[\"train\"].features[\"pos_tags\"].feature.names\n",
        "chunk_tags = dataset[\"train\"].features[\"chunk_tags\"].feature.names\n",
        "ner_tags = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "print(pos_tags)\n",
        "print(chunk_tags)\n",
        "print(ner_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZakjnJcQ8ePm",
        "outputId": "a88c1b69-8491-4c32-8063-77ace1441337"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
            "['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n",
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "LCAbbwEn9EEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokenized_dataset):\n",
        "        self.tokenized_dataset = tokenized_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenized_dataset[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "kE-280PC8g_p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    # Load the CoNLL-2003 dataset\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[\"tokens\"],\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label[word_idx])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                previous_word_idx = word_idx\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    # Tokenize datasets\n",
        "    tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "    return tokenized_datasets, len(label_list), label_list"
      ],
      "metadata": {
        "id": "NWvsvc_O9Fbi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Implementation"
      ],
      "metadata": {
        "id": "ie2vuYU_9YC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(NERModel, self).__init__()\n",
        "        self.bert = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "pYxLzkF_9H97"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=3, patience=2):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        val_loss = evaluate_model_loss(model, eval_dataloader, device)\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break"
      ],
      "metadata": {
        "id": "Bwv2NC1k9bur"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_loss(model, eval_dataloader, device):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "_aFw35xs9eVS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_dataloader, id2label, device):\n",
        "    model.eval()\n",
        "    true_labels, predicted_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            for label_ids, pred_ids in zip(labels, predictions):\n",
        "                true_labels_batch = [id2label[label.item()] for label in label_ids if label != -100]\n",
        "                predicted_labels_batch = [id2label[pred.item()] for pred, label in zip(pred_ids, label_ids) if label != -100]\n",
        "\n",
        "                if len(true_labels_batch) == len(predicted_labels_batch):\n",
        "                    true_labels.append(true_labels_batch)\n",
        "                    predicted_labels.append(predicted_labels_batch)\n",
        "\n",
        "    print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "eveiMqow9gLF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "2QhK8AVI9igP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "# Define your model class\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(NERModel, self).__init__()\n",
        "        self.bert = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "cA0qmaRp-3W1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load and prepare dataset\n",
        "    tokenized_datasets, num_labels, label_list = prepare_data()\n",
        "    id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "    train_dataset = NERDataset(tokenized_datasets[\"train\"])\n",
        "    eval_dataset = NERDataset(tokenized_datasets[\"validation\"])\n",
        "    test_dataset = NERDataset(tokenized_datasets[\"test\"])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=16)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "    # Model, optimizer, and scheduler\n",
        "    model = NERModel(num_labels=num_labels).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01, eps=1e-8)\n",
        "    num_epochs = 3\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_training_steps // 10, num_training_steps=num_training_steps)\n",
        "\n",
        "    # Training\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=num_epochs, patience=2)\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model_save_path = \"ner_model.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    evaluate_model(model, test_dataloader, id2label, device)\n",
        "\n",
        "# Run main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46i0zePL9h9s",
        "outputId": "9d78cc9e-0c96-421c-e101-8564f6e3bdbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "-yZKZKAt9qdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Specify the path to save the model\n",
        "model_path = '/content/ner_model.pth'\n",
        "\n",
        "def prepare_inference(model_path=model_path):\n",
        "    \"\"\"Initialize tokenizer and load model for inference\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Load the model architecture with correct num_labels\n",
        "    model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=9)\n",
        "\n",
        "    # Load the saved state dictionary\n",
        "    state_dict = torch.load(model_path, weights_only=True)\n",
        "    model.load_state_dict(state_dict, strict=False)  # strict=True for full load\n",
        "\n",
        "    # Define the label mapping\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    return tokenizer, model, id2label"
      ],
      "metadata": {
        "id": "H10nT3Yz9obi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, id2label):\n",
        "    \"\"\"\n",
        "    Perform NER inference on input text using CPU if CUDA is unavailable.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the text with offsets\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, return_offsets_mapping=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    offsets = inputs[\"offset_mapping\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    # Align predictions with words using offsets\n",
        "    labeled_words = []\n",
        "    for i, (start, end) in enumerate(offsets[0]):\n",
        "        if start == end:\n",
        "            continue\n",
        "        label = id2label[predictions[i]]\n",
        "        word = tokenizer.decode(input_ids[0][i:i + 1])\n",
        "        labeled_words.append((word, label))\n",
        "\n",
        "    return labeled_words"
      ],
      "metadata": {
        "id": "LxlDfuaWBDD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_entities(labeled_words):\n",
        "    \"\"\"Pretty print the labeled entities\"\"\"\n",
        "    current_entity = None\n",
        "    entity_text = []\n",
        "\n",
        "    for word, label in labeled_words:\n",
        "        if label == \"O\":\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = None\n",
        "                entity_text = []\n",
        "        elif label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "            current_entity = label[2:]\n",
        "            entity_text = [word]\n",
        "        elif label.startswith(\"I-\"):\n",
        "            if current_entity == label[2:]:\n",
        "                entity_text.append(word)\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = label[2:]\n",
        "                entity_text = [word]\n",
        "\n",
        "    if current_entity:  # Print last entity if exists\n",
        "        print(f\"{current_entity}: {' '.join(entity_text)}\")"
      ],
      "metadata": {
        "id": "7T4Y00l7BNfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model, id2label = prepare_inference()\n",
        "text = \"John Smith works at Microsoft in Seattle and visited New York last summer.\"\n",
        "print(\"Entities found:\")\n",
        "labeled_words = inference(text, model, tokenizer, id2label)\n",
        "print_entities(labeled_words)"
      ],
      "metadata": {
        "id": "kfT3C4IJBQKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre trained model"
      ],
      "metadata": {
        "id": "Jetu70dxBRwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "def prepare_inference(model_path=None):\n",
        "    \"\"\"Initialize tokenizer and load model for inference\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Load trained model if path provided, otherwise use a pre-trained model for NER\n",
        "    if model_path:\n",
        "        model = torch.load(model_path)\n",
        "    else:\n",
        "        # Always use a pre-trained model\n",
        "        model = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
        "\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    return tokenizer, model, id2label"
      ],
      "metadata": {
        "id": "SsBeONyBBUUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def inference(text, model, tokenizer, id2label):\n",
        "    \"\"\"\n",
        "    Perform NER inference on input text using CPU if CUDA is unavailable.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the text with offsets\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, return_offsets_mapping=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    offsets = inputs[\"offset_mapping\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    # Align predictions with words using offsets\n",
        "    labeled_words = []\n",
        "    for i, (start, end) in enumerate(offsets[0]):\n",
        "        if start == end:\n",
        "            continue\n",
        "        label = id2label[predictions[i]]\n",
        "        word = tokenizer.decode(input_ids[0][i:i + 1])\n",
        "        labeled_words.append((word, label))\n",
        "\n",
        "    return labeled_words"
      ],
      "metadata": {
        "id": "qo9hBJ4fBXaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_entities(labeled_words):\n",
        "    \"\"\"Pretty print the labeled entities\"\"\"\n",
        "    current_entity = None\n",
        "    entity_text = []\n",
        "\n",
        "    for word, label in labeled_words:\n",
        "        if label == \"O\":\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = None\n",
        "                entity_text = []\n",
        "        elif label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "            current_entity = label[2:]\n",
        "            entity_text = [word]\n",
        "        elif label.startswith(\"I-\"):\n",
        "            if current_entity == label[2:]:\n",
        "                entity_text.append(word)\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = label[2:]\n",
        "                entity_text = [word]\n",
        "\n",
        "    if current_entity:\n",
        "        print(f\"{current_entity}: {' '.join(entity_text)}\")"
      ],
      "metadata": {
        "id": "Rfvodfq2BZTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model, id2label = prepare_inference()\n",
        "text = \"John Smith works at Microsoft in Seattle and visited New York last summer.\"\n",
        "print(\"Entities found:\")\n",
        "labeled_words = inference(text, model, tokenizer, id2label)\n",
        "print_entities(labeled_words)"
      ],
      "metadata": {
        "id": "f2WwuG7gBa3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}